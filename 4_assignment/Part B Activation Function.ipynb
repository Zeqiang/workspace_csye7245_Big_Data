{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random, os, sys, math, json\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from sklearn import metrics\n",
    "# transfer learning using tf.slim, easy to load the original model structure and checkpoint\n",
    "# https://github.com/tensorflow/models/tree/master/research/slim\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.slim.nets as nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebookname = 'Assignment_4'\n",
    "\n",
    "# Generate 7 random char as instance running id\n",
    "run_id = random.sample('0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ', 7)\n",
    "run_id = \"\".join(x for x in run_id)\n",
    "# attampt_name is the name of the model\n",
    "attampt_name = 'Assignment_4_1_' + run_id\n",
    "\n",
    "# Path and Dir configration\n",
    "rootDir=''\n",
    "csvDataPath = os.path.join(rootDir, 'data_csv/')\n",
    "imageDataPath = os.path.join(rootDir,'data_images/')\n",
    "\n",
    "# Paramaters for model\n",
    "BATCH_SIZE = 20\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224\n",
    "NUM_CLASS = 7\n",
    "lesion_type_list = ['nv', 'mel', 'bkl', 'bcc', 'akiec', 'vasc', 'df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv data\n",
    "df_train_ori = pd.read_csv(os.path.join(csvDataPath, 'train.csv'))\n",
    "df_test_all = pd.read_csv(os.path.join(csvDataPath, 'test.csv'))\n",
    "\n",
    "# The number of last 3 types of images is still less (even after data augmentation), \n",
    "# replicate last 3 types of images to make training dataset more balanced\n",
    "df_lastTwo = df_train_ori.loc[df_train_ori['dx'].isin(['df', 'vasc'])].reset_index(drop=True)\n",
    "df_lastThree = df_train_ori.loc[df_train_ori['dx'].isin(['akiec'])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ori = df_train_ori.sample(frac=1).reset_index(drop=True).loc[:3000]\n",
    "df_test_all = df_test_all.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for input data\n",
    "def get_Labels(labels):\n",
    "    \"\"\"\n",
    "    ARG:\n",
    "        labels: list of actual label for a batch input\n",
    "    RETURN:\n",
    "        labelList: list of one-hot encoding labels\n",
    "    \"\"\"\n",
    "    labelList = np.zeros((len(labels),7))\n",
    "    for i in range(len(labels)):\n",
    "        labIdx = lesion_type_list.index(labels[i])\n",
    "        labelList[i][labIdx] = 1\n",
    "    return labelList\n",
    "\n",
    "# Get Next Batch\n",
    "def next_batch(indx1, indx2, csv_data):\n",
    "    \"\"\"\n",
    "    ARG:\n",
    "        indx1: start index for batch\n",
    "        indx2: end index for batch\n",
    "        csv_data: the dataframe of the all image information\n",
    "        ifTrain: True or False for if doing training process\n",
    "    RETURN:\n",
    "        imgPaths: list of path for batch images\n",
    "        imgLabels: list of labels for batch images\n",
    "        augFlags: list of augFlag for batch images\n",
    "    DESCRIPTION:\n",
    "        Get all information for one batch\n",
    "    \"\"\"\n",
    "    imgPaths = np.array(csv_data.path[indx1:indx2])\n",
    "    imgLabels = get_Labels(list(csv_data.dx[indx1:indx2]))\n",
    "    return imgPaths, imgLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Input Creation\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x_in = tf.placeholder(tf.string, shape=(None,), name='img_paths')\n",
    "y_label = tf.placeholder(tf.int32, shape=(None, NUM_CLASS), name='labels')\n",
    "\n",
    "\n",
    "def load_image(input_elems):\n",
    "    \"\"\"\n",
    "    ARG:\n",
    "        input_elems: one image path\n",
    "    RETURN:\n",
    "        image_result: the real image that is going to be passed into madel\n",
    "    \"\"\"\n",
    "    \n",
    "    image_file = input_elems\n",
    "    \n",
    "    image = tf.read_file(image_file)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    input_image = tf.cast(image, tf.float32)\n",
    "    # resizing to 224 x 224 x 3\n",
    "    image_resized = tf.image.resize_images(input_image, [IMG_HEIGHT, IMG_WIDTH], align_corners=True, \n",
    "                                           method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    return image_resized\n",
    "\n",
    "train_dataset = tf.map_fn(load_image, x_in, dtype=(tf.float32))\n",
    "image_inputs = tf.identity(train_dataset, name='new_inputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function for creating layers\n",
    "def create_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    " \n",
    "def create_biases(size):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[size]))\n",
    "\n",
    "def create_convolutional_layer(input, num_input_channels, conv_filter_size, num_filters):  \n",
    "    \n",
    "    ## We shall define the weights that will be trained using create_weights function.\n",
    "    weights = create_weights(shape=[conv_filter_size, conv_filter_size, num_input_channels, num_filters])\n",
    "    ## We create biases using the create_biases function. These are also trained.\n",
    "    biases = create_biases(num_filters)\n",
    "    ## Creating the convolutional layer\n",
    "    layer = tf.nn.conv2d(input=input, filter=weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    layer += biases\n",
    "    return layer\n",
    "\n",
    "def create_flatten_layer(layer):\n",
    "    layer_shape = layer.get_shape()\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    layer = tf.reshape(layer, [-1, num_features])\n",
    "    return layer\n",
    "\n",
    "def create_fc_layer(input, num_inputs, num_outputs, use_relu=True):\n",
    "    \n",
    "    #Let's define trainable weights and biases.\n",
    "    weights = create_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = create_biases(num_outputs)\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the activation function from tanh to relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CNN Model\n",
    "layer_conv1 = create_convolutional_layer(input=image_inputs, num_input_channels=3, conv_filter_size=7, num_filters=32)\n",
    "layer_conv1 = tf.nn.max_pool(value=layer_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "layer_conv1 = tf.nn.relu(layer_conv1)\n",
    "\n",
    "layer_conv2 = create_convolutional_layer(input=layer_conv1, num_input_channels=32, conv_filter_size=9, num_filters=64)\n",
    "layer_conv2 = tf.nn.max_pool(value=layer_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "layer_conv2 = tf.nn.relu(layer_conv2)\n",
    "\n",
    "layer_conv3 = create_convolutional_layer(input=layer_conv2, num_input_channels=64, conv_filter_size=11, num_filters=128)\n",
    "layer_conv3 = tf.nn.max_pool(value=layer_conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "layer_conv3 = tf.nn.relu(layer_conv3)\n",
    "          \n",
    "layer_flat = create_flatten_layer(layer_conv3)\n",
    " \n",
    "layer_fc = create_fc_layer(input=layer_flat, num_inputs=layer_flat.get_shape()[1:4].num_elements(), num_outputs=128)\n",
    " \n",
    "y_pred_logit = create_fc_layer(input=layer_fc, num_inputs=128, num_outputs=NUM_CLASS, use_relu=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output layer\n",
    "with tf.variable_scope('MyPrediction'):\n",
    "    y_pred_softmax = tf.nn.softmax(y_pred_logit, name='pred_softmax')\n",
    "    y_pred_cls = tf.argmax(y_pred_softmax, axis=1, name='pred_class')\n",
    "\n",
    "# Performance Measures \n",
    "y_label_cls = tf.cast(tf.argmax(y_label, axis=1), tf.int32, name='y_label_cls')\n",
    "correct_prediction = tf.equal(tf.cast(y_pred_cls, tf.int32), y_label_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='batch_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-1436f6bdba12>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cost-function\n",
    "mse_value = tf.losses.mean_squared_error(predictions=y_pred_logit, labels=y_label)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_logit, labels=y_label)\n",
    "loss_1 = tf.reduce_mean(cross_entropy, name='loss_1')\n",
    "loss_2 = tf.reduce_mean(mse_value, name='loss_2')\n",
    "\n",
    "# Optimization method\n",
    "myOptimizer_1 = tf.train.MomentumOptimizer(learning_rate=1e-4, momentum=0.9)\n",
    "myOptimizer_2 = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "train_op_1 = myOptimizer_1.minimize(loss_1, name='train_op_1')\n",
    "train_op_2 = myOptimizer_2.minimize(loss_2, name='train_op_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session creation and variables initialization\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_result():\n",
    "    \"\"\"\n",
    "    RETURN:\n",
    "        loss_over_all: the loss_1 for entire train data in model prediction\n",
    "        acc: the accuracy for entire train data in model prediction\n",
    "        cm: the confusion matrix for entire train data in model prediction\n",
    "    \"\"\"\n",
    "    num_train = len(df_train_ori)\n",
    "    train_label_cls = np.zeros(shape=num_train, dtype=np.int)\n",
    "    train_pred_cls = np.zeros(shape=num_train, dtype=np.int)\n",
    "    train_pred_prob = np.zeros(shape=[num_train,7])\n",
    "    \n",
    "    idx = 0\n",
    "    num_of_steps = 0\n",
    "    loss_value_all = 0.00\n",
    "    train_accuracy_all = 0.00\n",
    "    \n",
    "    while idx < num_train:\n",
    "        kidx = min(idx + BATCH_SIZE, num_train)\n",
    "        x_batch, y_true_batch = next_batch(idx, kidx, df_train_ori)\n",
    "        \n",
    "        loss_batch = sess.run(loss_1, feed_dict = {x_in:x_batch, y_label:y_true_batch})\n",
    "        acc_batch = sess.run(accuracy, feed_dict = {x_in:x_batch, y_label:y_true_batch})\n",
    "        \n",
    "        train_label_cls[idx:kidx] = sess.run(y_label_cls, feed_dict={y_label:y_true_batch})\n",
    "        train_pred_cls[idx:kidx] = sess.run(y_pred_cls, feed_dict={x_in:x_batch})\n",
    "        train_pred_prob[idx:kidx] = sess.run(y_pred_softmax, feed_dict={x_in:x_batch})\n",
    "        \n",
    "        loss_value_all += loss_batch\n",
    "        train_accuracy_all += acc_batch\n",
    "        idx = kidx\n",
    "        num_of_steps += 1\n",
    "        \n",
    "    loss_over_all = loss_value_all/num_of_steps\n",
    "    acc = train_accuracy_all/num_of_steps\n",
    "    cm = metrics.confusion_matrix(y_true=train_label_cls, y_pred=train_pred_cls)\n",
    "    \n",
    "    return loss_over_all, acc, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model, Showing the performance\n",
    "def print_test_result():\n",
    "    \"\"\"\n",
    "    RETURN:\n",
    "        loss_over_all: the loss_1 for entire test data in model prediction\n",
    "        acc: the accuracy for entire test data in model prediction\n",
    "        cm: the confusion matrix for entire test data in model prediction\n",
    "    \"\"\"\n",
    "    num_test = len(df_test_all)\n",
    "    test_label_cls = np.zeros(shape=num_test, dtype=np.int)\n",
    "    test_pred_cls = np.zeros(shape=num_test, dtype=np.int)\n",
    "    test_pred_prob = np.zeros(shape=[num_test,7])\n",
    "    \n",
    "    idx = 0\n",
    "    num_of_steps = 0\n",
    "    loss_value_all = 0.00\n",
    "    \n",
    "    while idx < num_test:\n",
    "        \n",
    "        kidx = min(idx + BATCH_SIZE, num_test)\n",
    "        x_batch, y_true_batch = next_batch(idx, kidx, df_test_all)\n",
    "        \n",
    "        loss_batch = sess.run(loss_1, feed_dict = {x_in:x_batch, y_label:y_true_batch})\n",
    "        acc_batch = sess.run(accuracy, feed_dict = {x_in:x_batch, y_label:y_true_batch})\n",
    "        \n",
    "        test_label_cls[idx:kidx] = sess.run(y_label_cls, feed_dict={y_label:y_true_batch})\n",
    "        test_pred_cls[idx:kidx] = sess.run(y_pred_cls, feed_dict={x_in:x_batch})\n",
    "        test_pred_prob[idx:kidx] = sess.run(y_pred_softmax, feed_dict={x_in:x_batch})\n",
    "        \n",
    "        loss_value_all += loss_batch\n",
    "        idx = kidx\n",
    "        num_of_steps += 1\n",
    "        \n",
    "    loss_over_all = loss_value_all/num_of_steps\n",
    "    correct = (test_label_cls == np.array(test_pred_cls))\n",
    "    correct_sum = np.array(correct).sum()\n",
    "    acc = float(correct_sum) / num_test\n",
    "\n",
    "    cm = metrics.confusion_matrix(y_true=test_label_cls, y_pred=test_pred_cls)\n",
    "    \n",
    "    # Print the accuracy.\n",
    "    msg = \"Accuracy on Test-Set: {0:.1%} ({1} / {2}),  loss: {3:>5.5}\"\n",
    "    msg_format = msg.format(acc, correct_sum, num_test, loss_over_all)\n",
    "    message_all = msg_format + '\\n' + str(cm)\n",
    "\n",
    "    print(message_all)\n",
    "    print('-------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file python3\n",
    "def print_to_write(file_name, msg_content, do):\n",
    "    \"\"\"\n",
    "    ARG:\n",
    "        file_name: name of report file\n",
    "        msg_content: message that is about to write into report file\n",
    "        do: 'a' or 'w+', appending or writing\n",
    "    DESCRIPTION:\n",
    "        Write message into report file\n",
    "    \"\"\"\n",
    "    logFile = open('report/assignment4/' + file_name + '.log', do)\n",
    "    print(msg_content, file=logFile)\n",
    "    print('-------------------------------------------------------------------------', file=logFile)\n",
    "    logFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myOptimizing():\n",
    "    \"\"\"\n",
    "    DESCRIPTION:\n",
    "        Entire training and testing optimization, print and save the result including model and performance\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    ############################################## Train for 10 Epochs ##############################################\n",
    "    for e in range(1,11):\n",
    "        start_time_e = time.time()\n",
    "\n",
    "        #shuffle order of data\n",
    "        train_csv_shuffle = df_train_ori.sample(frac=1).reset_index(drop=True)\n",
    "        num_train = len(train_csv_shuffle)\n",
    "        idx = 0\n",
    "        \n",
    "        while idx < num_train:\n",
    "            kidx = min(idx + BATCH_SIZE, num_train)\n",
    "            x_batch, y_true_batch = next_batch(idx, kidx, train_csv_shuffle)\n",
    "            sess.run(train_op_1, feed_dict = {x_in:x_batch, y_label:y_true_batch})\n",
    "            idx = kidx\n",
    "        \n",
    "        loss_over_all, acc, cm = train_result()\n",
    "        msg = 'Epochs:{0:>3},  Training Accuracy: {1:>6.1%},  loss: {2:>5.5}'\n",
    "        msg_format = msg.format(e, acc, loss_over_all)\n",
    "        message_all = msg_format + '\\n' + str(cm)\n",
    "\n",
    "        # print_to_write(attampt_name, message_all, 'a')\n",
    "        print(message_all)\n",
    "        print('-------------------------------------------------------------------------')\n",
    "        print_test_result()\n",
    "        \n",
    "        end_time_e = time.time()\n",
    "        time_dif_e = end_time_e - start_time_e\n",
    "        e_time = \"Time usage: \" + str(timedelta(seconds=int(round(time_dif_e))))\n",
    "        # print_to_write(attampt_name, e_time, 'a')\n",
    "        print(e_time)\n",
    "        print('-------------------------------------------------------------------------')\n",
    "    \n",
    "    # Total Time Usage\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    total_time = \"Total Trainning Time usage: \" + str(timedelta(seconds=int(round(time_dif))))\n",
    "    \n",
    "    # print_to_write(attampt_name, total_time, 'a')\n",
    "    print(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assignment_4_1_npBuz0D\n",
      "--------------------------- Starting Training ---------------------------\n",
      "Accuracy on Test-Set: 12.9% (9 / 70),  loss: 866.88\n",
      "[[0 1 0 2 7 0 0]\n",
      " [0 5 0 2 3 0 0]\n",
      " [0 5 0 0 5 0 0]\n",
      " [0 7 0 0 3 0 0]\n",
      " [0 6 0 0 4 0 0]\n",
      " [0 3 0 0 7 0 0]\n",
      " [0 7 0 0 3 0 0]]\n",
      "-------------------------------------------------------------------------\n",
      "Epochs:  1,  Training Accuracy:  14.3%,  loss:   nan\n",
      "[[40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]]\n",
      "-------------------------------------------------------------------------\n",
      "Accuracy on Test-Set: 14.3% (10 / 70),  loss:   nan\n",
      "[[10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]]\n",
      "-------------------------------------------------------------------------\n",
      "Time usage: 0:06:18\n",
      "-------------------------------------------------------------------------\n",
      "Epochs:  2,  Training Accuracy:  14.3%,  loss:   nan\n",
      "[[40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]]\n",
      "-------------------------------------------------------------------------\n",
      "Accuracy on Test-Set: 14.3% (10 / 70),  loss:   nan\n",
      "[[10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]]\n",
      "-------------------------------------------------------------------------\n",
      "Time usage: 0:06:45\n",
      "-------------------------------------------------------------------------\n",
      "Epochs:  3,  Training Accuracy:  14.3%,  loss:   nan\n",
      "[[40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]]\n",
      "-------------------------------------------------------------------------\n",
      "Accuracy on Test-Set: 14.3% (10 / 70),  loss:   nan\n",
      "[[10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]]\n",
      "-------------------------------------------------------------------------\n",
      "Time usage: 0:06:14\n",
      "-------------------------------------------------------------------------\n",
      "Epochs:  4,  Training Accuracy:  14.3%,  loss:   nan\n",
      "[[40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]]\n",
      "-------------------------------------------------------------------------\n",
      "Accuracy on Test-Set: 14.3% (10 / 70),  loss:   nan\n",
      "[[10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]]\n",
      "-------------------------------------------------------------------------\n",
      "Time usage: 0:06:43\n",
      "-------------------------------------------------------------------------\n",
      "Epochs:  5,  Training Accuracy:  14.3%,  loss:   nan\n",
      "[[40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]]\n",
      "-------------------------------------------------------------------------\n",
      "Accuracy on Test-Set: 14.3% (10 / 70),  loss:   nan\n",
      "[[10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]]\n",
      "-------------------------------------------------------------------------\n",
      "Time usage: 0:06:51\n",
      "-------------------------------------------------------------------------\n",
      "Epochs:  6,  Training Accuracy:  14.3%,  loss:   nan\n",
      "[[40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]]\n",
      "-------------------------------------------------------------------------\n",
      "Accuracy on Test-Set: 14.3% (10 / 70),  loss:   nan\n",
      "[[10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]]\n",
      "-------------------------------------------------------------------------\n",
      "Time usage: 0:06:27\n",
      "-------------------------------------------------------------------------\n",
      "Epochs:  7,  Training Accuracy:  14.3%,  loss:   nan\n",
      "[[40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]]\n",
      "-------------------------------------------------------------------------\n",
      "Accuracy on Test-Set: 14.3% (10 / 70),  loss:   nan\n",
      "[[10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]]\n",
      "-------------------------------------------------------------------------\n",
      "Time usage: 0:06:29\n",
      "-------------------------------------------------------------------------\n",
      "Epochs:  8,  Training Accuracy:  14.3%,  loss:   nan\n",
      "[[40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]]\n",
      "-------------------------------------------------------------------------\n",
      "Accuracy on Test-Set: 14.3% (10 / 70),  loss:   nan\n",
      "[[10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]]\n",
      "-------------------------------------------------------------------------\n",
      "Time usage: 0:06:34\n",
      "-------------------------------------------------------------------------\n",
      "Epochs:  9,  Training Accuracy:  14.3%,  loss:   nan\n",
      "[[40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]]\n",
      "-------------------------------------------------------------------------\n",
      "Accuracy on Test-Set: 14.3% (10 / 70),  loss:   nan\n",
      "[[10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]]\n",
      "-------------------------------------------------------------------------\n",
      "Time usage: 0:06:21\n",
      "-------------------------------------------------------------------------\n",
      "Epochs: 10,  Training Accuracy:  14.3%,  loss:   nan\n",
      "[[40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]\n",
      " [40  0  0  0  0  0  0]]\n",
      "-------------------------------------------------------------------------\n",
      "Accuracy on Test-Set: 14.3% (10 / 70),  loss:   nan\n",
      "[[10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0]]\n",
      "-------------------------------------------------------------------------\n",
      "Time usage: 0:06:58\n",
      "-------------------------------------------------------------------------\n",
      "Total Trainning Time usage: 1:05:40\n",
      "------------------------------- Complete --------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # print_to_write(attampt_name, attampt_name, 'w+')\n",
    "    # print_to_write(attampt_name, '--------------------------- Starting Training ---------------------------', 'a')\n",
    "    print(attampt_name)\n",
    "    print('--------------------------- Starting Training ---------------------------')\n",
    "    print_test_result()\n",
    "    myOptimizing()\n",
    "    print('------------------------------- Complete --------------------------------')\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change the activation function. How does it effect the accuracy?\n",
    "The accuracy was not effected so much by changing the activation function, however, the loss changed to a larger number by changing the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does it effect how quickly the network plateaus?\n",
    "The totoal training time of the previous one is 1:02:41 for 10 epochs, the new one is 1:05:40. It is not effected so much by changing the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
